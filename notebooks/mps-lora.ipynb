{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6497632",
      "metadata": {
        "id": "f6497632"
      },
      "source": [
        "# ðŸ“š Fine-Tuning with LoRA on Mac (MPS)\n",
        "This notebook guides you step-by-step through the process of fine-tuning a language model using the LoRA (Low-Rank Adaptation) technique on Mac with MPS (Metal Performance Shaders) acceleration.\n",
        "\n",
        "## ðŸŽ¯ What is LoRA?\n",
        "LoRA is an efficient technique for adapting pre-trained models that:\n",
        "\n",
        "- Drastically reduces the parameters to be trained (from billions to a few million)\n",
        "- Requires less GPU/RAM memory\n",
        "- Allows you to maintain multiple versions of the model (only the adapters change)\n",
        "- Produces results comparable to full fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“¦ Installing Libraries\n",
        "Install all the Python packages needed for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "687af238",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "687af238",
        "outputId": "87b705ff-e94c-41a4-95ff-7c3b108fb57c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.28.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading trl-0.28.0-py3-none-any.whl (540 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.28.0\n"
          ]
        }
      ],
      "source": [
        "# Installing the necessary libraries\n",
        "# Note: remove the ! if you are running from the terminal instead of Jupyter\n",
        "%pip install transformers datasets peft accelerate trl torch\n",
        "\n",
        "# (Optional) Verify versions)\n",
        "# %pip show transformers peft torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c2090f",
      "metadata": {
        "id": "77c2090f"
      },
      "source": [
        "## ðŸ“š Library Import and MPS Verification\n",
        "**What it does**: Imports all necessary libraries and verifies that Apple's MPS (Metal Performance Shaders) device is available and functioning.\n",
        "\n",
        "**MPS**: Apple's hardware accelerator for Macs with M1/M2/M3 chips, equivalent to CUDA for NVIDIA.\n",
        "\n",
        "âš ï¸ Recommendations:\n",
        "\n",
        "If MPS is not available, training will use the CPU (much slower).\n",
        "Make sure you have macOS 12.3+ and PyTorch 1.12+.\n",
        "If you see â€œMPS available: False,â€ check your PyTorch version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04af52ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04af52ce",
        "outputId": "572087f1-c5d7-4b1b-b557-fcaead3cb334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "CHECK MPS DEVICE\n",
            "==================================================\n",
            "âœ“ MPS available: False\n",
            "âœ“ MPS built in PyTorch: False\n",
            "âš ï¸  Selected device: CPU (training will be slower)\n",
            "âœ“ PyTorch version: 2.9.0+cpu\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# LIBRARY IMPORT\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,      # Class for causal models (GPT, Llama, etc.)\n",
        "    AutoTokenizer,              # Automatic tokenizer\n",
        "    TrainingArguments,          # Training configuration\n",
        "    Trainer,                    # HuggingFace trainer class\n",
        "    DataCollatorForLanguageModeling  # Prepares batches for language modeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,                 # LoRA configuration\n",
        "    get_peft_model,             # Applies LoRA to the model\n",
        "    TaskType                    # Task type (CAUSAL_LM, SEQ_2_SEQ, etc.)\n",
        ")\n",
        "from datasets import load_dataset  # Load dataset from HuggingFace Hub\n",
        "import os\n",
        "\n",
        "FINE_TUNED_MODEL_NAME = os.getenv(\"FINE_TUNED_MODEL_NAME\", \"\")\n",
        "\n",
        "# ============================================\n",
        "# CHECK MPS DEVICE\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"CHECK MPS DEVICE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if MPS is available\n",
        "mps_available = torch.backends.mps.is_available()\n",
        "mps_built = torch.backends.mps.is_built()\n",
        "\n",
        "print(f\"âœ“ MPS available: {mps_available}\")\n",
        "print(f\"âœ“ MPS built in PyTorch: {mps_built}\")\n",
        "\n",
        "# Select the best available device\n",
        "if mps_available:\n",
        "    device = torch.device(\"mps\")\n",
        "    print(f\"âœ“ Selected device: MPS (Apple Silicon)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"âš ï¸  Selected device: CPU (training will be slower)\")\n",
        "\n",
        "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fc9c81",
      "metadata": {
        "id": "e9fc9c81"
      },
      "source": [
        "## ðŸ¤– Model and Tokenizer Loading\n",
        "**What it does:** Loads the pre-trained language model and its tokenizer from HuggingFace Hub.\n",
        "\n",
        "**Components:**\n",
        "\n",
        "- **Tokenizer:** Converts text into numbers (token IDs) that the model can process.\n",
        "- **Model:** The pre-trained neural network that will be fine-tuned.\n",
        "\n",
        "**âš ï¸ Recommendations:**\n",
        "\n",
        "- Start with small models (500M/1B parameters) for testing\n",
        "- Larger models require more RAM\n",
        "- For Llama or Mistral (7B parameters), you need at least 16GB of unified RAM\n",
        "- torch.float16 halves memory usage compared to float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "564d5ea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "564d5ea4",
        "outputId": "4b88d26d-22e3-420c-f9fb-00b7ebd1991b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "LOADING MODEL: meta-llama/Llama-3.2-1B-Instruct\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n401 Client Error. (Request ID: Root=1-69944205-157a45c17363ca9a4bd77580;412e99d9-a726-4872-838a-5d882ad8758b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '401 Unauthorized' for url 'https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhead_call_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder, retry_on_errors)\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1692\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint, retry_on_errors)\u001b[0m\n\u001b[1;32m   1613\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m     response = _httpx_follow_relative_redirects(\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_on_errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_httpx_follow_relative_redirects\u001b[0;34m(method, url, retry_on_errors, **httpx_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    676\u001b[0m             )\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-69944205-4055282848fbc3a672db0e0c;e8fec705-df08-4ced-a75c-ef3899a34dce)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    618\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    484\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n401 Client Error. (Request ID: Root=1-69944205-4055282848fbc3a672db0e0c;e8fec705-df08-4ced-a75c-ef3899a34dce)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '401 Unauthorized' for url 'https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhead_call_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1803\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder, retry_on_errors)\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1692\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint, retry_on_errors)\u001b[0m\n\u001b[1;32m   1613\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m     response = _httpx_follow_relative_redirects(\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_on_errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_httpx_follow_relative_redirects\u001b[0;34m(method, url, retry_on_errors, **httpx_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    676\u001b[0m             )\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-69944205-157a45c17363ca9a4bd77580;412e99d9-a726-4872-838a-5d882ad8758b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2069703580.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# The tokenizer converts text into token IDs and vice versa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m tokenizer= AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;31m# Allows custom code (necessary for some models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 )\n\u001b[1;32m    620\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0mconfig_model_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"revision\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_config_key\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_config_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_config_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    484\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n401 Client Error. (Request ID: Root=1-69944205-157a45c17363ca9a4bd77580;412e99d9-a726-4872-838a-5d882ad8758b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MODEL CONFIG\n",
        "# ============================================\n",
        "\n",
        "# Choose the model to fine-tune\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Example: \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"LOADING MODEL: {model_name}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TOKENIZER LOADING\n",
        "# ============================================\n",
        "\n",
        "# The tokenizer converts text into token IDs and vice versa\n",
        "tokenizer= AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True  # Allows custom code (necessary for some models)\n",
        ")\n",
        "\n",
        "# This is necessary for batching during training\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"âœ“ Pad token set to EOS token\")\n",
        "\n",
        "# ============================================\n",
        "# MODEL LOADING\n",
        "# ============================================\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.float16,  # Use reduced precision to save memory\n",
        "    device_map={\"\": device},     # Map the model to the selected device\n",
        "    trust_remote_code=True       # Allows custom code\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# MODEL INFORMATION\n",
        "# ============================================\n",
        "\n",
        "# Count the total parameters in the model\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"âœ“ Model loaded: {model_name}\")\n",
        "print(f\"âœ“ Total parameters: {total_params:,}\")\n",
        "print(f\"âœ“ Trainable parameters (pre-LoRA): {trainable_params:,}\")\n",
        "print(f\"âœ“ Vocabulary size: {len(tokenizer):,}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b21a1e",
      "metadata": {
        "id": "87b21a1e"
      },
      "source": [
        "### ðŸ” Model Structure Inspection (Optional but Recommended)\n",
        "**What it does**: Analyzes the internal structure of the model to identify which layers can be targeted by LoRA.\n",
        "\n",
        "**Why it matters**: Each model architecture has different names for its layers. LoRA needs to know which layers to modify (typically the attention layers).\n",
        "\n",
        "**âš ï¸ Recommendations**:\n",
        "\n",
        "- Run this cell the first time you use a new model\n",
        "- Helps understand the model structure\n",
        "- Useful for debugging if LoRA cannot find the target modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc92f14",
      "metadata": {
        "id": "1dc92f14",
        "outputId": "fb6c3fbf-5e8c-4890-dcd8-8f2b151b0d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "MODEL STRUCTURE ANALYSIS\n",
            "==================================================\n",
            "\n",
            "ðŸ“‹ Linear modules found (candidates for LoRA):\n",
            "\n",
            "  â€¢ model.layers.0.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.0.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.0.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.0.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.0.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.0.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.0.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.1.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.1.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.1.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.1.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.1.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.1.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.1.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.2.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.2.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.2.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.2.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.2.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.2.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.2.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.3.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.3.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.3.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.3.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.3.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.3.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.3.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.4.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.4.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.4.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.4.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.4.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.4.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.4.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.5.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.5.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.5.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.5.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.5.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.5.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.5.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.6.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.6.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.6.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.6.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.6.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.6.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.6.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.7.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.7.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.7.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.7.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.7.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.7.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.7.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.8.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.8.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.8.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.8.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.8.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.8.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.8.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.9.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.9.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.9.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.9.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.9.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.9.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.9.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.10.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.10.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.10.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.10.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.10.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.10.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.10.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.11.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.11.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.11.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.11.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.11.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.11.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.11.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.12.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.12.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.12.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.12.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.12.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.12.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.12.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.13.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.13.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.13.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.13.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.13.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.13.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.13.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.14.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.14.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.14.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.14.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.14.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.14.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.14.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ model.layers.15.self_attn.q_proj â†’ type: Linear\n",
            "  â€¢ model.layers.15.self_attn.k_proj â†’ type: Linear\n",
            "  â€¢ model.layers.15.self_attn.v_proj â†’ type: Linear\n",
            "  â€¢ model.layers.15.self_attn.o_proj â†’ type: Linear\n",
            "  â€¢ model.layers.15.mlp.gate_proj â†’ type: Linear\n",
            "  â€¢ model.layers.15.mlp.up_proj â†’ type: Linear\n",
            "  â€¢ model.layers.15.mlp.down_proj â†’ type: Linear\n",
            "  â€¢ lm_head â†’ type: Linear\n",
            "\n",
            "âœ“ Linear modules found: {'gate_proj', 'v_proj', 'down_proj', 'up_proj', 'k_proj', 'lm_head', 'q_proj', 'o_proj'}\n",
            "\n",
            "ðŸ’¡ These are the names to use in 'target_modules' for LoRA\n",
            "\n",
            "âœ… Recommended modules for LoRA: {'gate_proj', 'v_proj', 'down_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj'}\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# INSPECT MODEL STRUCTURE\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"MODEL STRUCTURE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ============================================\n",
        "# FIND LINEAR MODULES (CANDIDATES FOR LORA)\n",
        "# ============================================\n",
        "\n",
        "# LoRA works better on linear layers (fully connected)\n",
        "# These are typically the attention and projection layers\n",
        "\n",
        "print(\"\\nðŸ“‹ Linear modules found (candidates for LoRA):\\n\")\n",
        "\n",
        "linear_modules = set()\n",
        "for name, module in model.named_modules():\n",
        "    # Find all linear layers\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        # Extract the module name (last part of the path)\n",
        "        module_name = name.split('.')[-1]\n",
        "        linear_modules.add(module_name)\n",
        "\n",
        "        # Show first 10 examples\n",
        "        if len([m for m in model.named_modules() if m[0] == name]) <= 10:\n",
        "            print(f\"  â€¢ {name} â†’ type: {type(module).__name__}\")\n",
        "\n",
        "# ============================================\n",
        "# RELEVANT MODULES FOR LORA\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nâœ“ Linear modules found: {linear_modules}\")\n",
        "print(\"\\nðŸ’¡ These are the names to use in 'target_modules' for LoRA\")\n",
        "\n",
        "# Common modules to exclude (embedding and output layers)\n",
        "exclude_modules = {'lm_head', 'embed_tokens', 'wte', 'wpe', 'ln_f'}\n",
        "recommended_modules = linear_modules - exclude_modules\n",
        "\n",
        "print(f\"\\nâœ… Recommended modules for LoRA: {recommended_modules}\")\n",
        "print(\"\\n\" + \"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f5f42a",
      "metadata": {
        "id": "37f5f42a"
      },
      "source": [
        "## âš™ï¸ LoRA Configuration and Application\n",
        "**What it does**: Configures LoRA parameters and applies adapters to the model. Only LoRA adapters will be trained, not the entire model.\n",
        "\n",
        "**LoRA parameters explained**:\n",
        "\n",
        "- `r (rank)`: Size of LoRA matrices. Typical values: 8-64. Higher = more capacity but more parameters\n",
        "- `lora_alpha`: Scaling factor. Typically 2Ã—r. Controls how much the adapters influence\n",
        "- `target_modules`: Which layers to modify (attention, projections, etc.)\n",
        "- `lora_dropout`: Dropout for regularization (0.05-0.1)\n",
        "- `bias`: Whether to also train biases (â€œnoneâ€, â€˜allâ€™, â€œlora_onlyâ€)\n",
        "\n",
        "**âš ï¸ Recommendations**:\n",
        "\n",
        "- Start with r=8 or r=16 for quick tests\n",
        "- r=32 or r=64 for better results but slower training\n",
        "- If you have limited memory, reduce r\n",
        "- For models from ~7B on unified 16 GB: start with r=8, alpha=16, and very small batches\n",
        "- The automatic function finds the correct modules for any model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196797ca",
      "metadata": {
        "id": "196797ca",
        "outputId": "9ab2cba1-80e4-484d-b6de-1cfc3eaf42df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LoRA CONFIGURATION\n",
            "==================================================\n",
            "\n",
            "âœ“ Target modules identified: ['gate_proj', 'v_proj', 'down_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj']\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "#   FUNCTION TO FIND TARGET MODULES AUTOMATICALLY\n",
        "# ============================================\n",
        "\n",
        "def find_target_modules(model, exclude_names=None):\n",
        "    \"\"\"\n",
        "    Identify candidate module names to apply LoRA to by scanning the model for\n",
        "    linear layers.\n",
        "\n",
        "    The function collects the last component of each module's dotted name\n",
        "    (e.g. \"transformer.h.0.attn.c_attn\" -> \"c_attn\") and returns a list of\n",
        "    unique module names, excluding common non-target modules like embeddings\n",
        "    and the output head.\n",
        "\n",
        "    Args:\n",
        "        model: A torch.nn.Module (the loaded model) to inspect.\n",
        "        exclude_names: Optional set of module names to exclude (strings).\n",
        "                       If None, a sensible default set is used.\n",
        "\n",
        "    Returns:\n",
        "        A list of module names (strings) suitable for use as `target_modules`\n",
        "        in a LoRA configuration.\n",
        "    \"\"\"\n",
        "    if exclude_names is None:\n",
        "        # Common modules to exclude (embedding layers and output head)\n",
        "        exclude_names = {'lm_head', 'embed_tokens', 'wte', 'wpe', 'ln_f'}\n",
        "\n",
        "    target_modules = set()\n",
        "\n",
        "    # Find all linear layers in the model\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Extract the module name (last part of the path)\n",
        "            # E.g. \"transformer.h.0.attn.c_attn\" â†’ \"c_attn\"\n",
        "            module_name = name.split('.')[-1]\n",
        "            if module_name:  # skip empty root names\n",
        "                target_modules.add(module_name)\n",
        "\n",
        "    # Remove excluded names\n",
        "    target_modules = target_modules - exclude_names\n",
        "\n",
        "    return list(target_modules)\n",
        "\n",
        "# ============================================\n",
        "#   FUNCTION TO FIND TARGET MODULES AUTOMATICALLY\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"LoRA CONFIGURATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "target_modules = find_target_modules(model)\n",
        "print(f\"\\nâœ“ Target modules identified: {target_modules}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22765476",
      "metadata": {
        "id": "22765476",
        "outputId": "a520e0cf-1493-4946-cdc1-d3e1d9dd3110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š LoRA Configuration :\n",
            "  â€¢ Rank (r): 16\n",
            "  â€¢ Alpha: 32\n",
            "  â€¢ Dropout: 0.05\n",
            "  â€¢ Target modules: {'gate_proj', 'v_proj', 'down_proj', 'up_proj', 'k_proj', 'q_proj', 'o_proj'}\n",
            "  â€¢ Task type: CAUSAL_LM\n",
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matteo/Desktop/rnd/lora/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Gradient checkpointing enabled\n",
            "âœ“ Input require grads enabled\n",
            "âœ“ Cache disabled\n",
            "\n",
            "==================================================\n",
            "PARAMETER STATISTICS\n",
            "==================================================\n",
            "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# LORA PARAMETERS CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "# Create LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    # Rank of LoRA matrices (dimensionality of decomposition)\n",
        "    # Typical values: 8 (fast), 16 (balanced), 32-64 (best results)\n",
        "    r=16,\n",
        "\n",
        "    # Alpha: scaling factor for LoRA adapters\n",
        "    # General rule: alpha = 2 * r\n",
        "    # Controls the intensity of the adaptation\n",
        "    lora_alpha=32,\n",
        "\n",
        "    # Modules to modify with LoRA\n",
        "    # Typically attention and projection layers\n",
        "    target_modules=target_modules,\n",
        "\n",
        "    # Dropout for regularization (prevents overfitting)\n",
        "    # Typical values: 0.05-0.1\n",
        "    lora_dropout=0.05,\n",
        "\n",
        "    # Whether to train biases as well\n",
        "    # \"none\": do not train biases (more efficient)\n",
        "    # \"all\": train all biases\n",
        "    # \"lora_only\": train only LoRA layer biases\n",
        "    bias=\"none\",\n",
        "\n",
        "    # Task type: CAUSAL_LM for autoregressive models (GPT, Llama)\n",
        "    # Others: SEQ_2_SEQ_LM, SEQ_CLS, TOKEN_CLS, QUESTION_ANS\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š LoRA Configuration :\")\n",
        "print(f\"  â€¢ Rank (r): {lora_config.r}\")\n",
        "print(f\"  â€¢ Alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"  â€¢ Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"  â€¢ Target modules: {lora_config.target_modules}\")\n",
        "print(f\"  â€¢ Task type: {lora_config.task_type}\")\n",
        "\n",
        "# ============================================\n",
        "# APPLY LORA TO MODEL\n",
        "# ============================================\n",
        "\n",
        "# Wrap the base model with LoRA adapters\n",
        "# Only the adapters will be trained, the rest remains frozen\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# ============================================\n",
        "# ENABLE MEMORY SAVING TECHNIQUES\n",
        "# ============================================\n",
        "\n",
        "# Use getattr/setattr to avoid Pylance errors\n",
        "# Gradient checkpointing: reduces peak memory during training\n",
        "gradient_checkpointing_fn = getattr(model, \"gradient_checkpointing_enable\", None)\n",
        "if gradient_checkpointing_fn is not None and callable(gradient_checkpointing_fn):\n",
        "    gradient_checkpointing_fn()\n",
        "    print(\"âœ“ Gradient checkpointing enabled\")\n",
        "\n",
        "# Enable input require grads: necessary for gradient checkpointing with LoRA\n",
        "enable_grads_fn = getattr(model, \"enable_input_require_grads\", None)\n",
        "if enable_grads_fn is not None and callable(enable_grads_fn):\n",
        "    enable_grads_fn()\n",
        "    print(\"âœ“ Input require grads enabled\")\n",
        "\n",
        "# Disable cache to save memory during training\n",
        "if hasattr(model, \"config\") and hasattr(model.config, \"use_cache\"):\n",
        "    setattr(model.config, \"use_cache\", False)\n",
        "    print(\"âœ“ Cache disabled\")\n",
        "\n",
        "# ============================================\n",
        "# PARAMETER STATISTICS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PARAMETER STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "model.print_trainable_parameters()\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Reduction is typically 99%+\n",
        "# E.g. from 124M parameters to ~300K trainable parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823d2ac2",
      "metadata": {
        "id": "823d2ac2"
      },
      "source": [
        "## ðŸ“Š Dataset Preparation\n",
        "**What it does**: Loads a dataset, tokenizes it (converts text into numbers), and splits it into training and validation sets.\n",
        "\n",
        "**Process**:\n",
        "\n",
        "1. Load the dataset from HuggingFace Hub\n",
        "2. Tokenize the text (convert to token IDs)\n",
        "3. Apply padding and truncation to standardize lengths\n",
        "4. Divide into train/validation split\n",
        "\n",
        "âš ï¸ **Recommendations**:\n",
        "\n",
        "- For quick tests, use a small subset (e.g., `split=â€œtrain[:1000]â€`)\n",
        "- `max_length=512` is a good compromise. Higher values require more memory\n",
        "- For your custom dataset, replace `load_dataset` with your own data loading\n",
        "- Make sure the dataset has a â€œtextâ€ column or adapt the tokenization function\n",
        "\n",
        "Recommended datasets for initial testing:\n",
        "\n",
        "- `imdb`: Movie reviews (sentiment analysis), used in this notebook\n",
        "- `wikitext`: Text from Wikipedia\n",
        "- `openwebtext`: Text from the web\n",
        "- Your custom dataset in CSV/JSON format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c54c40d",
      "metadata": {
        "id": "3c54c40d",
        "outputId": "ee0f4a14-a600-4a45-c99f-0f92de4a221f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DATASET PREPARATION\n",
            "==================================================\n",
            "\n",
            "âœ“ Dataset loaded: IMDB\n",
            "âœ“ Number of examples: 1000\n",
            "âœ“ Available columns: ['text', 'label']\n",
            "\n",
            "ðŸ“ Example text:\n",
            "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ev...\n",
            "\n",
            "ðŸ”„ Tokenization in progress...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 6088.09 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Tokenization completed\n",
            "\n",
            "âœ“ Split completed:\n",
            "  â€¢ Training samples: 900\n",
            "  â€¢ Validation samples: 100\n",
            "\n",
            "ðŸ“Š Tokenization statistics:\n",
            "  â€¢ Max length: 512 tokens\n",
            "  â€¢ Padding: max_length\n",
            "  â€¢ Truncation: enabled\n",
            "\n",
            "ðŸ”¢ Tokenization example:\n",
            "  â€¢ Input IDs shape: 512\n",
            "  â€¢ First 10 token IDs: [128000, 1, 21119, 426, 1, 374, 26549, 555, 10826, 32294]\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# LOAD DATASET\n",
        "# ============================================\n",
        "\n",
        "from typing import cast\n",
        "from datasets import Dataset  # Import for type hint\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"DATASET PREPARATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load a sample dataset from HuggingFace Hub\n",
        "# IMDB contains movie reviews (positive/negative)\n",
        "# For quick tests, we only use 1000 examples\n",
        "dataset = cast(\n",
        "    Dataset,\n",
        "    load_dataset(\n",
        "        \"imdb\",\n",
        "        split=\"train[:1000]\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Dataset loaded: IMDB\")\n",
        "print(f\"âœ“ Number of examples: {len(dataset)}\")\n",
        "print(f\"âœ“ Available columns: {dataset.column_names}\")\n",
        "\n",
        "# Show an example\n",
        "print(f\"\\nðŸ“ Example text:\")\n",
        "example_text = dataset[0][\"text\"]  # Explicit access, more type-safe\n",
        "if isinstance(example_text, str):\n",
        "    print(f\"{example_text[:200]}...\")\n",
        "\n",
        "# ============================================\n",
        "# TOKENIZATION FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes the dataset text.\n",
        "\n",
        "    Args:\n",
        "        examples: Batch of examples from the dataset\n",
        "\n",
        "    Returns:\n",
        "        Dict with input_ids, attention_mask, etc.\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],        # Column containing the text\n",
        "        truncation=True,         # Truncate texts longer than max_length\n",
        "        max_length=512,          # Maximum length in tokens\n",
        "        padding=\"max_length\",    # Pad all examples to max_length\n",
        "        return_tensors=None      # Return lists, not tensors (for now)\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# DATASET TOKENIZATION\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nðŸ”„ Tokenization in progress...\")\n",
        "\n",
        "# Apply tokenization to the entire dataset\n",
        "# batched=True processes multiple examples at once (faster)\n",
        "tokenized_dataset: Dataset = dataset.map(  # Type hint for Pylance\n",
        "    tokenize_function,           # Function to apply\n",
        "    batched=True,                # Process in batch (more efficient)\n",
        "    remove_columns=list(dataset.column_names),  # Explicit cast to list\n",
        "    desc=\"Tokenizing dataset\"    # Description for progress bar\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Tokenization completed\")\n",
        "\n",
        "# ============================================\n",
        "# TRAIN/VALIDATION SPLIT\n",
        "# ============================================\n",
        "\n",
        "# Split the dataset into training and validation\n",
        "# The validation set is used to monitor overfitting\n",
        "train_test_split = tokenized_dataset.train_test_split(\n",
        "    test_size=0.1,    # 10% for validation, 90% for training\n",
        "    seed=42           # Seed for reproducibility\n",
        ")\n",
        "\n",
        "train_dataset: Dataset = train_test_split[\"train\"]  # Explicit type hint\n",
        "eval_dataset: Dataset = train_test_split[\"test\"]    # Explicit type hint\n",
        "\n",
        "print(f\"\\nâœ“ Split completed:\")\n",
        "print(f\"  â€¢ Training samples: {len(train_dataset)}\")\n",
        "print(f\"  â€¢ Validation samples: {len(eval_dataset)}\")\n",
        "\n",
        "# ============================================\n",
        "# TOKENIZATION INFORMATION\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nðŸ“Š Tokenization statistics:\")\n",
        "print(f\"  â€¢ Max length: 512 tokens\")\n",
        "print(f\"  â€¢ Padding: max_length\")\n",
        "print(f\"  â€¢ Truncation: enabled\")\n",
        "\n",
        "# Show a tokenized example\n",
        "print(f\"\\nðŸ”¢ Tokenization example:\")\n",
        "print(f\"  â€¢ Input IDs shape: {len(train_dataset[0]['input_ids'])}\")\n",
        "print(f\"  â€¢ First 10 token IDs: {train_dataset[0]['input_ids'][:10]}\")\n",
        "\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f5a5fe",
      "metadata": {
        "id": "22f5a5fe"
      },
      "source": [
        "## ðŸŽ¯ Training Configuration\n",
        "**What it does**: Configures all parameters for training: batch size, learning rate, number of epochs, saving strategies, logging, etc.\n",
        "\n",
        "**Key parameters explained**:\n",
        "\n",
        "**Batch Size and Gradient Accumulation**:\n",
        "\n",
        "- `per_device_train_batch_size`: How many examples to process together (higher = faster but more memory)\n",
        "- `gradient_accumulation_steps`: Accumulates gradients for N steps before updating weights (simulates larger batch sizes)\n",
        "- Effective batch size = `per_device_train_batch_size` Ã— `gradient_accumulation_steps`\n",
        "\n",
        "**Learning Rate**:\n",
        "\n",
        "- `learning_rate`: How fast the model learns (2e-4 is a good default for LoRA)\n",
        "- Too high: unstable training\n",
        "- Too low: very slow training\n",
        "\n",
        "**Strategies**:\n",
        "\n",
        "- `evaluation_strategy`: When to evaluate the model (â€œstepsâ€, â€˜epochâ€™, â€œnoâ€)\n",
        "- `save_strategy`: When to save checkpoints (â€œstepsâ€, â€˜epochâ€™, â€œnoâ€)\n",
        "\n",
        "âš ï¸ **Recommendations**:\n",
        "\n",
        "- If you have memory issues, reduce `per_device_train_batch_size` to 1 or 2\\\n",
        "- Increase `gradient_accumulation_steps` to compensate for small batch sizes\n",
        "- `num_train_epochs=3` is a good starting point\n",
        "- Monitor the loss: if it doesn't decrease, increase the learning rate or epochs\n",
        "- Enable gradient checkpointing when working with models >3B: it consumes less memory but increases the time per step\n",
        "- For a 7B model on a Mac with 16 GB: use `per_device_train_batch_size=1`, `gradient_accumulation_steps=8`, `max_length=384`, `r=8`, `lora_alpha=16`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "191ad432",
      "metadata": {
        "id": "191ad432",
        "outputId": "0b7be15a-1a3e-4cba-eaae-bf9416632a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "TRAINING CONFIGURATION\n",
            "==================================================\n",
            "\n",
            "ðŸ“Š Configuration summary:\n",
            "\n",
            "  Training:\n",
            "    â€¢ Epochs: 3\n",
            "    â€¢ Batch size (per device): 4\n",
            "    â€¢ Gradient accumulation: 4\n",
            "    â€¢ Effective batch size: 16\n",
            "    â€¢ Learning rate: 0.0002\n",
            "    â€¢ Warmup steps: 100\n",
            "\n",
            "  Evaluation:\n",
            "    â€¢ Strategy: steps\n",
            "    â€¢ Every: 100 steps\n",
            "\n",
            "  Saving:\n",
            "    â€¢ Strategy: steps\n",
            "    â€¢ Every: 100 steps\n",
            "    â€¢ Total checkpoints: 3\n",
            "\n",
            "  Device:\n",
            "    â€¢ FP16: False\n",
            "\n",
            "  Memory:\n",
            "    â€¢ Gradient checkpointing: True\n",
            "\n",
            "  Estimated duration:\n",
            "    â€¢ Steps per epoch: ~56\n",
            "    â€¢ Total steps: ~168\n",
            "\n",
            "âœ“ Training configuration completed\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# DATA COLLATOR\n",
        "# ============================================\n",
        "\n",
        "# The data collator prepares batches during training\n",
        "# For language modeling, it automatically handles labels\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # False = Causal LM (predicts next token)\n",
        "               # True = Masked LM (predicts masked tokens, for BERT)\n",
        ")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ============================================\n",
        "# TRAINING ARGUMENTS\n",
        "# ============================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # ----------------------------------------\n",
        "    # PATHS AND OUTPUT\n",
        "    # ----------------------------------------\n",
        "    output_dir=f\"./{FINE_TUNED_MODEL_NAME}-finetuned\",      # Where to save checkpoints       # Where to save TensorBoard logs\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # TRAINING DURATION\n",
        "    # ----------------------------------------\n",
        "    num_train_epochs=3,                  # Number of epochs (full passes over the dataset)\n",
        "                                         # 3-5 epochs are typically enough for LoRA\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # BATCH SIZE AND GRADIENT ACCUMULATION\n",
        "    # ----------------------------------------\n",
        "    per_device_train_batch_size=4,       # Batch size per device during training\n",
        "                                         # Reduce to 2 or 1 if you have memory issues\n",
        "\n",
        "    per_device_eval_batch_size=4,        # Batch size per device during evaluation\n",
        "                                         # Can be higher than train batch size\n",
        "\n",
        "    gradient_accumulation_steps=4,       # Accumulate gradients for N steps\n",
        "                                         # Effective batch size = 4 Ã— 4 = 16\n",
        "                                         # Useful to simulate large batch sizes\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # LEARNING RATE AND OPTIMIZATION\n",
        "    # ----------------------------------------\n",
        "    learning_rate=2e-4,                  # Learning rate (2e-4 is standard for LoRA)\n",
        "                                         # Typical range: 1e-5 to 5e-4\n",
        "\n",
        "    warmup_steps=100,                    # Warmup steps (gradually increase LR)\n",
        "                                         # Helps stability at the start of training\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # MEMORY SAVING TECHNIQUES\n",
        "    # ----------------------------------------\n",
        "    gradient_checkpointing=True,         # Save memory by trading off with compute time\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # PRECISION AND DEVICE\n",
        "    # ----------------------------------------\n",
        "    fp16=False,                          # Do not use mixed precision float16\n",
        "                                         # MPS does not natively support fp16\n",
        "\n",
        "    bf16=False,                          # Do not use bfloat16\n",
        "                                         # MPS does not support bf16\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # LOGGING AND MONITORING\n",
        "    # ----------------------------------------\n",
        "    logging_steps=10,                    # Log metrics every N steps\n",
        "                                         # Lower = more detailed but slower\n",
        "\n",
        "    report_to=\"none\",                    # Where to send logs\n",
        "                                         # \"none\": nowhere\n",
        "                                         # \"tensorboard\": TensorBoard\n",
        "                                         # \"wandb\": Weights & Biases\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # EVALUATION\n",
        "    # ----------------------------------------\n",
        "    eval_strategy=\"steps\",               # When to evaluate: \"steps\", \"epoch\", \"no\"\n",
        "    eval_steps=100,                      # Evaluate every 100 steps\n",
        "                                         # Useful to monitor overfitting\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # CHECKPOINT SAVING\n",
        "    # ----------------------------------------\n",
        "    save_strategy=\"steps\",               # When to save: \"steps\", \"epoch\", \"no\"\n",
        "    save_steps=100,                      # Save checkpoint every 100 steps\n",
        "    save_total_limit=3,                  # Keep only the last 3 checkpoints\n",
        "                                         # Saves disk space\n",
        "\n",
        "    load_best_model_at_end=True,         # Load the best model at the end\n",
        "                                         # Based on eval_loss\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # OTHER OPTIONS\n",
        "    # ----------------------------------------\n",
        "    remove_unused_columns=False,         # Do not remove extra columns from the dataset\n",
        "\n",
        "    # Optional: seed for reproducibility\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION SUMMARY\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nðŸ“Š Configuration summary:\")\n",
        "print(f\"\\n  Training:\")\n",
        "print(f\"    â€¢ Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"    â€¢ Batch size (per device): {training_args.per_device_train_batch_size}\")\n",
        "print(f\"    â€¢ Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"    â€¢ Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"    â€¢ Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"    â€¢ Warmup steps: {training_args.warmup_steps}\")\n",
        "\n",
        "print(f\"\\n  Evaluation:\")\n",
        "print(f\"    â€¢ Strategy: {training_args.eval_strategy}\")\n",
        "print(f\"    â€¢ Every: {training_args.eval_steps} steps\")\n",
        "\n",
        "print(f\"\\n  Saving:\")\n",
        "print(f\"    â€¢ Strategy: {training_args.save_strategy}\")\n",
        "print(f\"    â€¢ Every: {training_args.save_steps} steps\")\n",
        "print(f\"    â€¢ Total checkpoints: {training_args.save_total_limit}\")\n",
        "\n",
        "print(f\"\\n  Device:\")\n",
        "print(f\"    â€¢ FP16: {training_args.fp16}\")\n",
        "print(f\"\\n  Memory:\")\n",
        "print(f\"    â€¢ Gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
        "\n",
        "# Calculate total steps\n",
        "steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "\n",
        "print(f\"\\n  Estimated duration:\")\n",
        "print(f\"    â€¢ Steps per epoch: ~{steps_per_epoch}\")\n",
        "print(f\"    â€¢ Total steps: ~{total_steps}\")\n",
        "\n",
        "print(\"\\nâœ“ Training configuration completed\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ca2dc09",
      "metadata": {
        "id": "9ca2dc09"
      },
      "source": [
        "## ðŸš€ Model Training\n",
        "**What it does**: Initializes the HuggingFace Trainer and starts the fine-tuning process. This is the cell that actually performs the training.\n",
        "\n",
        "**What happens during training**:\n",
        "\n",
        "1. The model processes batches of data\n",
        "2. Calculates the loss (prediction error)\n",
        "3. Calculates gradients via backpropagation\n",
        "4. Updates only the LoRA parameters (not the entire model)\n",
        "5. Periodically evaluates on the validation set\n",
        "6. Saves checkpoints\n",
        "\n",
        "âš ï¸ **Recommendations**:\n",
        "\n",
        "- Training can take from minutes to hours (depending on the dataset and hardware)\n",
        "- Monitor the loss: it should decrease gradually\n",
        "- If the training loss decreases but the evaluation loss increases = overfitting\n",
        "- You can stop with the stop button and resume from the checkpoint\n",
        "- The first epoch is slower (MPS compilation)\n",
        "\n",
        "**Metrics to monitor:**\n",
        "\n",
        "- `loss`: Error on the training set (should decrease)\n",
        "- `eval_loss`: Error on the validation set (should decrease, but not too much compared to loss)\n",
        "- `learning_rate`: Changes during warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e9219d",
      "metadata": {
        "id": "c0e9219d",
        "outputId": "a05071e1-3bb5-4890-8a12-b4f5762a1a49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "TRAINER INITIALIZATION\n",
            "==================================================\n",
            "âœ“ Trainer initialized\n",
            "âœ“ Model: meta-llama/Llama-3.2-1B-Instruct\n",
            "âœ“ Training samples: 900\n",
            "âœ“ Validation samples: 100\n",
            "\n",
            "==================================================\n",
            "ðŸš€ START TRAINING\n",
            "==================================================\n",
            "\n",
            "â³ Training has started...\n",
            "ðŸ’¡ Monitor the metrics below:\n",
            "   â€¢ loss: error on the training set (should decrease)\n",
            "   â€¢ eval_loss: error on the validation set\n",
            "   â€¢ learning_rate: current learning rate\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matteo/Desktop/rnd/lora/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 46:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.212300</td>\n",
              "      <td>3.258708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matteo/Desktop/rnd/lora/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# TRAINER INITIALIZATION\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"TRAINER INITIALIZATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# The Trainer is the HuggingFace class that handles the entire training loop\n",
        "trainer = Trainer(\n",
        "    model=model,                    # The model with LoRA applied\n",
        "    args=training_args,             # Training configuration\n",
        "    train_dataset=train_dataset,    # Training dataset\n",
        "    eval_dataset=eval_dataset,      # Validation dataset\n",
        "    data_collator=data_collator,    # Prepares batches\n",
        ")\n",
        "\n",
        "print(\"âœ“ Trainer initialized\")\n",
        "print(f\"âœ“ Model: {model_name}\")\n",
        "print(f\"âœ“ Training samples: {len(train_dataset)}\")\n",
        "print(f\"âœ“ Validation samples: {len(eval_dataset)}\")\n",
        "\n",
        "# ============================================\n",
        "# START TRAINING\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸš€ START TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nâ³ Training has started...\")\n",
        "print(\"ðŸ’¡ Monitor the metrics below:\")\n",
        "print(\"   â€¢ loss: error on the training set (should decrease)\")\n",
        "print(\"   â€¢ eval_loss: error on the validation set\")\n",
        "print(\"   â€¢ learning_rate: current learning rate\")\n",
        "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Start training\n",
        "# This may take several minutes/hours\n",
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcce30b",
      "metadata": {
        "id": "8dcce30b",
        "outputId": "0cb14902-ce6e-464a-bd38-83cc29c43a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "âœ… TRAINING COMPLETED!\n",
            "==================================================\n",
            "\n",
            "ðŸ“Š Final training statistics:\n",
            "  â€¢ Training loss: 3.1574\n",
            "  â€¢ Training steps: 171\n",
            "  â€¢ Total time: 2814.69 seconds\n",
            "  â€¢ Samples/second: 0.96\n",
            "\n",
            "ðŸ” Final evaluation on the validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matteo/Desktop/rnd/lora/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ˆ Validation results:\n",
            "  â€¢ Eval loss: 3.2587\n",
            "  â€¢ Perplexity: 26.02\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# TRAINING COMPLETED\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"âœ… TRAINING COMPLETED!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Show final training statistics\n",
        "print(f\"\\nðŸ“Š Final training statistics:\")\n",
        "print(f\"  â€¢ Training loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  â€¢ Training steps: {train_result.global_step}\")\n",
        "print(f\"  â€¢ Total time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"  â€¢ Samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# FINAL EVALUATION\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nðŸ” Final evaluation on the validation set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Validation results:\")\n",
        "print(f\"  â€¢ Eval loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"  â€¢ Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n",
        "\n",
        "# Perplexity: Measure of how \"confused\" the model is\n",
        "# Lower = better. Typical values: 10-50 for small models\n",
        "\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c5d9ab",
      "metadata": {
        "id": "83c5d9ab"
      },
      "source": [
        "## ðŸ’¾ Model Saving\n",
        "**What it does**: Saves the fine-tuned model (LoRA adapters only) and tokenizer to disk.\n",
        "\n",
        "âš ï¸ **Recommendations**:\n",
        "\n",
        "- LoRA adapters are very lightweight (a few MB vs. GB for the full model)\n",
        "- You can save multiple versions with different names\n",
        "- To share the model, you also need the original base model\n",
        "- Important backup: save to the cloud or an external drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34389825",
      "metadata": {
        "id": "34389825",
        "outputId": "e2c9a025-4b0f-4c33-c04b-6528a6299557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "MODEL SAVING\n",
            "==================================================\n",
            "\n",
            "ðŸ’¾ Saving in progress...\n",
            "ðŸ“ Path: ./lora-finetuned-imdb-finetuned\n",
            "âœ“ LoRA adapters saved\n",
            "âœ“ Tokenizer saved\n",
            "\n",
            "ðŸ“‚ Saved files:\n",
            "  â€¢ adapter_model.safetensors (43.03 MB)\n",
            "  â€¢ tokenizer_config.json (0.05 MB)\n",
            "  â€¢ special_tokens_map.json (0.00 MB)\n",
            "  â€¢ checkpoint-100 (0.00 MB)\n",
            "  â€¢ checkpoint-171 (0.00 MB)\n",
            "  â€¢ tokenizer.json (16.41 MB)\n",
            "  â€¢ README.md (0.00 MB)\n",
            "  â€¢ adapter_config.json (0.00 MB)\n",
            "  â€¢ chat_template.jinja (0.00 MB)\n",
            "\n",
            "âœ“ Total size: 59.50 MB\n",
            "\n",
            "ðŸ’¡ Note: Only LoRA adapters are saved (~60 MB)\n",
            "   The base model (meta-llama/Llama-3.2-1B-Instruct) must be loaded separately\n",
            "\n",
            "==================================================\n",
            "âœ… SAVING COMPLETED\n",
            "==================================================\n",
            "\n",
            "ðŸ“– To load this model in the future:\n",
            "\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "from peft import PeftModel\n",
            "\n",
            "# Load base model\n",
            "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
            "\n",
            "# Load LoRA adapter\n",
            "model = PeftModel.from_pretrained(base_model, \"./lora-finetuned-imdb-finetuned\")\n",
            "\n",
            "# Load tokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"./lora-finetuned-imdb-finetuned\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# SAVE FINE-TUNED MODEL\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"MODEL SAVING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Path to save the model\n",
        "output_dir = f\"./{FINE_TUNED_MODEL_NAME}-finetuned\"\n",
        "\n",
        "print(f\"\\nðŸ’¾ Saving in progress...\")\n",
        "print(f\"ðŸ“ Path: {output_dir}\")\n",
        "\n",
        "# ============================================\n",
        "# SAVE LORA ADAPTER\n",
        "# ============================================\n",
        "\n",
        "# Save only the LoRA adapters (few MB)\n",
        "# The base model is not saved (saves space)\n",
        "model.save_pretrained(output_dir)\n",
        "print(f\"âœ“ LoRA adapters saved\")\n",
        "\n",
        "# ============================================\n",
        "# SAVE TOKENIZER\n",
        "# ============================================\n",
        "\n",
        "# Also save the tokenizer (needed for inference)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"âœ“ Tokenizer saved\")\n",
        "\n",
        "# ============================================\n",
        "# SAVED FILE INFORMATION\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "\n",
        "print(f\"\\nðŸ“‚ Saved files:\")\n",
        "for file in os.listdir(output_dir):\n",
        "    file_path = os.path.join(output_dir, file)\n",
        "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    print(f\"  â€¢ {file} ({size_mb:.2f} MB)\")\n",
        "\n",
        "# Calculate total size\n",
        "total_size = sum(\n",
        "    os.path.getsize(os.path.join(output_dir, f))\n",
        "    for f in os.listdir(output_dir)\n",
        ") / (1024 * 1024)\n",
        "\n",
        "print(f\"\\nâœ“ Total size: {total_size:.2f} MB\")\n",
        "print(f\"\\nðŸ’¡ Note: Only LoRA adapters are saved (~{total_size:.0f} MB)\")\n",
        "print(f\"   The base model ({model_name}) must be loaded separately\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"âœ… SAVING COMPLETED\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ============================================\n",
        "# INSTRUCTIONS TO LOAD THE MODEL\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nðŸ“– To load this model in the future:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"{model_name}\")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"{output_dir}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{output_dir}\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6414aac2",
      "metadata": {
        "id": "6414aac2"
      },
      "source": [
        "## ðŸ§ª Model Testing and Inference\n",
        "**What it does**: Loads the fine-tuned model and generates text to test the training results.\n",
        "\n",
        "**Generation parameters**:\n",
        "\n",
        "- `max_new_tokens`: How many new tokens to generate\n",
        "- `temperature`: Randomness *(0.1 = conservative, 1.0 = creative, 2.0 = very random)*\n",
        "- `top_p`: Nucleus sampling *(0.9 = use top 90% probability)*\n",
        "- `top_k`: Consider only the top K most probable tokens\n",
        "- `do_sample`: True = probabilistic sampling, False = greedy (always the most probable)\n",
        "\n",
        "âš ï¸ Recommendations:\n",
        "\n",
        "- Test with various prompts to evaluate the model\n",
        "- Compare with the base model (without LoRA) to see the improvement\n",
        "- Experiment with different temperatures for different results\n",
        "  - Low temperature *(0.3-0.7)* = more consistent\n",
        "  - High temperature *(0.8-1.2)* = more creative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd90e46e",
      "metadata": {
        "id": "dd90e46e",
        "outputId": "0ad815e4-5e45-44b5-9ea5-444bb68b2e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "FINE-TUNED MODEL TEST\n",
            "==================================================\n",
            "\n",
            "ðŸ”„ Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Base model loaded: meta-llama/Llama-3.2-1B-Instruct\n",
            "âœ“ LoRA adapters loaded from: ./lora-finetuned-imdb-finetuned\n",
            "âœ“ Tokenizer loaded\n",
            "\n",
            "==================================================\n",
            "ðŸ§ª GENERATION EXAMPLES\n",
            "==================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "ðŸ“ Prompt: 'This movie was absolutely'\n",
            "ðŸ¤– Generated: 'terrible. I can't believe the people who made this were so dumb they thought it would be funny to make a movie about a guy who falls in love with his boss's wife.<br /><br />I have seen better movies than this one.'\n",
            "ðŸ“„ Complete: 'This movie was absolutely terrible. I can't believe the people who made this were so dumb they thought it would be funny to make a movie about a guy who falls in love with his boss's wife.<br /><br />I have seen better movies than this one.'\n",
            "\n",
            "--- Example 2 ---\n",
            "ðŸ“ Prompt: 'I really enjoyed'\n",
            "ðŸ¤– Generated: 'this movie. The plot was very interesting, and the acting wasn't bad at all. I liked how they kept it real throughout the whole movie (no one seems to be a complete fake). It's not like any other movie ever made that is'\n",
            "ðŸ“„ Complete: 'I really enjoyed this movie. The plot was very interesting, and the acting wasn't bad at all. I liked how they kept it real throughout the whole movie (no one seems to be a complete fake). It's not like any other movie ever made that is'\n",
            "\n",
            "--- Example 3 ---\n",
            "ðŸ“ Prompt: 'The plot was'\n",
            "ðŸ¤– Generated: 'pretty thin, but I still managed to enjoy it. The acting and special effects were okay for the time period. If you're a fan of zombie movies or are looking for something like that to watch on a rainy day, this might be a good'\n",
            "ðŸ“„ Complete: 'The plot was pretty thin, but I still managed to enjoy it. The acting and special effects were okay for the time period. If you're a fan of zombie movies or are looking for something like that to watch on a rainy day, this might be a good'\n",
            "\n",
            "--- Example 4 ---\n",
            "ðŸ“ Prompt: 'The acting performance'\n",
            "ðŸ¤– Generated: 'of the cast was not impressive, but they were supposed to be a group of young actors. In this case, I don't know if it's because they are so young that they didn't have enough experience or whether their performances just weren't good'\n",
            "ðŸ“„ Complete: 'The acting performance of the cast was not impressive, but they were supposed to be a group of young actors. In this case, I don't know if it's because they are so young that they didn't have enough experience or whether their performances just weren't good'\n",
            "\n",
            "==================================================\n",
            "ðŸ’¬ INTERACTIVE TEST\n",
            "==================================================\n",
            "\n",
            "ðŸ’¡ Try your own custom prompts!\n",
            "   Edit the variable 'custom_prompt' below:\n",
            "\n",
            "ðŸ“ Custom prompt: 'This film is a masterpiece because'\n",
            "\n",
            "ðŸ¤– Generated text:\n",
            "This film is a masterpiece because of the way it was made. It's not about any particular story, but how to make an audience feel something and keep them on their toes throughout. This film is one big thrill ride that keeps you guessing what will happen next.<br /><br />The actors are good, but they're so over-the-top that it makes no sense. I don't know who would have thought that someone could play such different characters and still be good at it! The special effects were great, but when combined\n",
            "\n",
            "==================================================\n",
            "ðŸ”¬ COMPARISON: Fine-tuned vs Base\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ Prompt: 'This movie was'\n",
            "\n",
            "ðŸ”µ BASE MODEL:\n",
            "This movie was released in 1995, but it has gained popularity in recent years. It is a science fiction film directed by Robert Zemeckis, starring Tom Hanks and Meg Ryan. The movie is set in a dystopian future where a global pandemic\n",
            "\n",
            "ðŸŸ¢ FINE-TUNED MODEL:\n",
            "This movie was so bad, I could not stand to watch it again. The special effects were awful and the plot just seemed like a bunch of garbage thrown together. There is no way this movie should have been rated for adult content! It's really funny how much\n",
            "\n",
            "ðŸ’¡ Notice the differences in style and content!\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# LOAD MODEL FOR INFERENCE\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"FINE-TUNED MODEL TEST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "from peft import PeftModel\n",
        "from typing import cast\n",
        "\n",
        "print(f\"\\nðŸ”„ Loading model...\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD BASE MODEL\n",
        "# ============================================\n",
        "\n",
        "# Reload the base model (without LoRA)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": device},\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Base model loaded: {model_name}\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD LORA ADAPTER\n",
        "# ============================================\n",
        "\n",
        "# Load the fine-tuned LoRA adapters\n",
        "model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "model.eval()  # Set to evaluation mode (disable dropout)\n",
        "\n",
        "# Cast for type checking - model is PeftModel which has base_model\n",
        "peft_model = cast(PeftModel, model)\n",
        "if hasattr(peft_model, 'base_model') and hasattr(peft_model.base_model, 'config'):\n",
        "    peft_model.base_model.config.use_cache = True  # type: ignore[attr-defined]\n",
        "\n",
        "print(f\"âœ“ LoRA adapters loaded from: {output_dir}\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD TOKENIZER\n",
        "# ============================================\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "print(f\"âœ“ Tokenizer loaded\")\n",
        "\n",
        "# ============================================\n",
        "# GENERATION FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def generate_text(prompt: str, max_new_tokens: int = 100, temperature: float = 0.7,\n",
        "                  top_p: float = 0.9, top_k: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    Generate text from a prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: Initial text\n",
        "        max_new_tokens: Maximum number of tokens to generate\n",
        "        temperature: Controls randomness (0.1-2.0)\n",
        "        top_p: Nucleus sampling (0.0-1.0)\n",
        "        top_k: Top-k sampling\n",
        "\n",
        "    Returns:\n",
        "        Generated text\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt and move directly to device\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = cast(torch.Tensor, encoded[\"input_ids\"]).to(device)\n",
        "    attention_mask = cast(torch.Tensor, encoded[\"attention_mask\"]).to(device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():  # Disable gradient calculation (faster)\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,           # Use sampling (not greedy)\n",
        "            top_p=top_p,              # Nucleus sampling\n",
        "            top_k=top_k,              # Top-k sampling\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1,   # Penalize repetitions\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# ============================================\n",
        "# TEST WITH VARIOUS PROMPTS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ§ª GENERATION EXAMPLES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# List of test prompts\n",
        "test_prompts = [\n",
        "    \"This movie was absolutely\",\n",
        "    \"I really enjoyed\",\n",
        "    \"The plot was\",\n",
        "    \"The acting performance\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(f\"ðŸ“ Prompt: '{prompt}'\")\n",
        "\n",
        "    # Generate text\n",
        "    generated = generate_text(\n",
        "        prompt,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Extract only the generated part (remove the prompt)\n",
        "    generated_only = generated[len(prompt):].strip()\n",
        "\n",
        "    print(f\"ðŸ¤– Generated: '{generated_only}'\")\n",
        "    print(f\"ðŸ“„ Complete: '{generated}'\")\n",
        "\n",
        "# ============================================\n",
        "# INTERACTIVE TEST\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ’¬ INTERACTIVE TEST\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nðŸ’¡ Try your own custom prompts!\")\n",
        "print(\"   Edit the variable 'custom_prompt' below:\\n\")\n",
        "\n",
        "# Edit this prompt with your own text\n",
        "custom_prompt = \"This film is a masterpiece because\"\n",
        "\n",
        "print(f\"ðŸ“ Custom prompt: '{custom_prompt}'\")\n",
        "\n",
        "# Generate with customizable parameters\n",
        "generated = generate_text(\n",
        "    custom_prompt,\n",
        "    max_new_tokens=100,      # Generate up to 100 tokens\n",
        "    temperature=0.7,         # Balanced between coherence and creativity\n",
        "    top_p=0.9,              # Nucleus sampling\n",
        "    top_k=50                # Top-k sampling\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ¤– Generated text:\")\n",
        "print(f\"{generated}\")\n",
        "\n",
        "# ============================================\n",
        "# COMPARISON WITH BASE MODEL (OPTIONAL)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ”¬ COMPARISON: Fine-tuned vs Base\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load base model without LoRA for comparison\n",
        "base_model_only = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": device}\n",
        ")\n",
        "base_model_only.eval()\n",
        "\n",
        "comparison_prompt = \"This movie was\"\n",
        "\n",
        "print(f\"\\nðŸ“ Prompt: '{comparison_prompt}'\")\n",
        "\n",
        "# Generate with base model - with explicit casts\n",
        "encoded_base = tokenizer(comparison_prompt, return_tensors=\"pt\")\n",
        "input_ids_base = cast(torch.Tensor, encoded_base[\"input_ids\"]).to(device)\n",
        "attention_mask_base = cast(torch.Tensor, encoded_base[\"attention_mask\"]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    base_output = base_model_only.generate(\n",
        "        input_ids=input_ids_base,\n",
        "        attention_mask=attention_mask_base,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "base_text = tokenizer.decode(base_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate with fine-tuned model\n",
        "finetuned_text = generate_text(comparison_prompt, max_new_tokens=50)\n",
        "\n",
        "print(f\"\\nðŸ”µ BASE MODEL:\")\n",
        "print(f\"{base_text}\")\n",
        "\n",
        "print(f\"\\nðŸŸ¢ FINE-TUNED MODEL:\")\n",
        "print(f\"{finetuned_text}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Notice the differences in style and content!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3ddc4b",
      "metadata": {
        "id": "9a3ddc4b"
      },
      "source": [
        "## ðŸ”— Merge Adapters (Optional)\n",
        "**What it does**: Combines LoRA adapters with the base model to create a complete standalone model.\n",
        "\n",
        "**When to use merge**:\n",
        "\n",
        "- âœ… For production deployment (faster)\n",
        "- âœ… To share the complete model\n",
        "- âœ… If you don't need to switch between different adapters\n",
        "\n",
        "**When NOT to use merge**:\n",
        "\n",
        "- âŒ If you want to keep multiple LoRA versions for the same base model\n",
        "- âŒ If disk space is limited (merging creates a complete model)\n",
        "- âŒ If you want to continue fine-tuning\n",
        "\n",
        "âš ï¸ **Recommendations**:\n",
        "\n",
        "- The merged model is larger (size of the complete base model)\n",
        "- Inference is slightly faster (no LoRA overhead)\n",
        "- You can no longer edit only the adapters after merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2473f50",
      "metadata": {
        "id": "c2473f50",
        "outputId": "057b758d-bae9-4e04-bb85-470b55ce809e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "MERGE LORA ADAPTER\n",
            "==================================================\n",
            "\n",
            "ðŸ”„ Merging in progress...\n",
            "ðŸ’¡ This combines the LoRA adapters with the base model\n",
            "âœ“ Merge completed\n",
            "\n",
            "ðŸ’¾ Saving merged model...\n",
            "ðŸ“ Path: ./lora-finetuned-imdb-merged\n",
            "âœ“ Merged model saved\n",
            "\n",
            "ðŸ“Š Size comparison:\n",
            "  â€¢ LoRA Adapter: 350.89 MB\n",
            "  â€¢ Merged Model: 2373.61 MB\n",
            "  â€¢ Difference: 2022.72 MB\n",
            "\n",
            "==================================================\n",
            "ADAPTER vs MERGED\n",
            "==================================================\n",
            "\n",
            "âœ… Advantages of ADAPTER (not merged):\n",
            "  â€¢ Smaller size (~351 MB)\n",
            "  â€¢ You can have multiple adapters for the same base model\n",
            "  â€¢ Easy to share and version\n",
            "\n",
            "âœ… Advantages of MERGED:\n",
            "  â€¢ Slightly faster inference\n",
            "  â€¢ Easier to deploy (single model)\n",
            "  â€¢ No need to load base model + adapter\n",
            "\n",
            "ðŸ’¡ Recommendation:\n",
            "  â€¢ Use ADAPTER for development and experimentation\n",
            "  â€¢ Use MERGED for production deployment\n",
            "\n",
            "==================================================\n",
            "TEST MERGED MODEL\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ Prompt: 'This movie was absolutely'\n",
            "ðŸ¤– Generated: 'This movie was absolutely awful. It had no plot, no characters, no action, no suspense. It was just a blank piece of paper with a title. The acting was also extremely poor. I have never seen such bad acting in any movie. I wish that I'\n",
            "\n",
            "âœ“ The merged model works correctly!\n",
            "\n",
            "==================================================\n",
            "âœ… PROCESS COMPLETED\n",
            "==================================================\n",
            "\n",
            "ðŸ“– To load the merged model in the future:\n",
            "\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "# Load merged model (no need for PeftModel)\n",
            "model = AutoModelForCausalLM.from_pretrained(\"./lora-finetuned-imdb-merged\")\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"./lora-finetuned-imdb-merged\")\n",
            "\n",
            "# Use normally for inference\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MERGE LORA ADAPTER WITH BASE MODEL\n",
        "# ============================================\n",
        "\n",
        "from typing import cast\n",
        "import os\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"MERGE LORA ADAPTER\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\nðŸ”„ Merging in progress...\")\n",
        "print(f\"ðŸ’¡ This combines the LoRA adapters with the base model\")\n",
        "\n",
        "# ============================================\n",
        "# EXECUTE MERGE\n",
        "# ============================================\n",
        "\n",
        "# Merge: combines LoRA weights with base model weights\n",
        "# Result: a complete model without separate adapters\n",
        "merged_model = model.merge_and_unload() # type: ignore[attr-defined]\n",
        "\n",
        "print(f\"âœ“ Merge completed\")\n",
        "\n",
        "# ============================================\n",
        "# SAVE MERGED MODEL\n",
        "# ============================================\n",
        "\n",
        "merged_output_dir = f\"./{FINE_TUNED_MODEL_NAME}-merged\"\n",
        "\n",
        "print(f\"\\nðŸ’¾ Saving merged model...\")\n",
        "print(f\"ðŸ“ Path: {merged_output_dir}\")\n",
        "\n",
        "# Save the merged model (full size)\n",
        "merged_model.save_pretrained(merged_output_dir)\n",
        "tokenizer.save_pretrained(merged_output_dir)\n",
        "\n",
        "print(f\"âœ“ Merged model saved\")\n",
        "\n",
        "# ============================================\n",
        "# SIZE COMPARISON\n",
        "# ============================================\n",
        "\n",
        "def get_dir_size(path: str) -> float:\n",
        "    \"\"\"Calculate total size of a directory\"\"\"\n",
        "    total = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            total += os.path.getsize(filepath)\n",
        "    return total / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "adapter_size = get_dir_size(output_dir)\n",
        "merged_size = get_dir_size(merged_output_dir)\n",
        "\n",
        "print(f\"\\nðŸ“Š Size comparison:\")\n",
        "print(f\"  â€¢ LoRA Adapter: {adapter_size:.2f} MB\")\n",
        "print(f\"  â€¢ Merged Model: {merged_size:.2f} MB\")\n",
        "print(f\"  â€¢ Difference: {merged_size - adapter_size:.2f} MB\")\n",
        "\n",
        "# ============================================\n",
        "# ADVANTAGES AND DISADVANTAGES\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(\"ADAPTER vs MERGED\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\nâœ… Advantages of ADAPTER (not merged):\")\n",
        "print(f\"  â€¢ Smaller size (~{adapter_size:.0f} MB)\")\n",
        "print(f\"  â€¢ You can have multiple adapters for the same base model\")\n",
        "print(f\"  â€¢ Easy to share and version\")\n",
        "\n",
        "print(f\"\\nâœ… Advantages of MERGED:\")\n",
        "print(f\"  â€¢ Slightly faster inference\")\n",
        "print(f\"  â€¢ Easier to deploy (single model)\")\n",
        "print(f\"  â€¢ No need to load base model + adapter\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Recommendation:\")\n",
        "print(f\"  â€¢ Use ADAPTER for development and experimentation\")\n",
        "print(f\"  â€¢ Use MERGED for production deployment\")\n",
        "\n",
        "# ============================================\n",
        "# TEST MERGED MODEL\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(\"TEST MERGED MODEL\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load the merged model for testing\n",
        "merged_model_test = AutoModelForCausalLM.from_pretrained(\n",
        "    merged_output_dir,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": device}\n",
        ")\n",
        "merged_model_test.eval()\n",
        "\n",
        "test_prompt = \"This movie was absolutely\"\n",
        "\n",
        "# Tokenize and move to device - CORRECT with cast\n",
        "encoded_test = tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "input_ids_test = cast(torch.Tensor, encoded_test[\"input_ids\"]).to(device)\n",
        "attention_mask_test = cast(torch.Tensor, encoded_test[\"attention_mask\"]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = merged_model_test.generate(\n",
        "        input_ids=input_ids_test,\n",
        "        attention_mask=attention_mask_test,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"\\nðŸ“ Prompt: '{test_prompt}'\")\n",
        "print(f\"ðŸ¤– Generated: '{generated}'\")\n",
        "\n",
        "print(f\"\\nâœ“ The merged model works correctly!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"âœ… PROCESS COMPLETED\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ============================================\n",
        "# INSTRUCTIONS TO LOAD MERGED MODEL\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\nðŸ“– To load the merged model in the future:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load merged model (no need for PeftModel)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{merged_output_dir}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{merged_output_dir}\")\n",
        "\n",
        "# Use normally for inference\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c3d6e1",
      "metadata": {
        "id": "16c3d6e1"
      },
      "source": [
        "## ðŸ“Š Analysis and Visualization\n",
        "*What it does*: Analyzes training metrics and creates visualizations to better understand progress.\n",
        "âš ï¸ **Recommendations**:\n",
        "- Useful for understanding whether training went well\n",
        "- Helps identify overfitting or underfitting\n",
        "- Use to decide whether to do more epochs or change hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7925c818",
      "metadata": {
        "id": "7925c818",
        "outputId": "7aa0fe8b-d3d4-49ff-d431-2f5cdc2fd101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "TRAINING ANALYSIS\n",
            "==================================================\n",
            "\n",
            "ðŸ“Š Training statistics:\n",
            "  â€¢ Total training steps: 17\n",
            "  â€¢ Evaluation steps: 2\n",
            "  â€¢ Initial loss: 3.3689\n",
            "  â€¢ Final loss: 3.0121\n",
            "  â€¢ Improvement: 10.6%\n",
            "  â€¢ Best eval loss: 3.2587\n",
            "  â€¢ Final eval loss: 3.2587\n",
            "\n",
            "âœ“ Chart saved at: ./training_metrics.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DIAGNOSIS\n",
            "==================================================\n",
            "\n",
            "ðŸ“ˆ Overfitting analysis:\n",
            "  â€¢ Final training loss: 3.0121\n",
            "  â€¢ Final validation loss: 3.2587\n",
            "  â€¢ Gap: 0.2466\n",
            "  âš ï¸  Slight overfitting, but acceptable\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# TRAINING METRICS ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"TRAINING ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ============================================\n",
        "# LOAD HISTORY FROM TRAINER\n",
        "# ============================================\n",
        "\n",
        "# The trainer saves all metrics during training\n",
        "history = trainer.state.log_history\n",
        "\n",
        "# Extract loss for training and validation\n",
        "train_losses = [entry['loss'] for entry in history if 'loss' in entry]\n",
        "eval_losses = [entry['eval_loss'] for entry in history if 'eval_loss' in entry]\n",
        "\n",
        "print(f\"\\nðŸ“Š Training statistics:\")\n",
        "print(f\"  â€¢ Total training steps: {len(train_losses)}\")\n",
        "print(f\"  â€¢ Evaluation steps: {len(eval_losses)}\")\n",
        "\n",
        "if train_losses:\n",
        "    print(f\"  â€¢ Initial loss: {train_losses[0]:.4f}\")\n",
        "    print(f\"  â€¢ Final loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  â€¢ Improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")\n",
        "\n",
        "if eval_losses:\n",
        "    print(f\"  â€¢ Best eval loss: {min(eval_losses):.4f}\")\n",
        "    print(f\"  â€¢ Final eval loss: {eval_losses[-1]:.4f}\")\n",
        "\n",
        "# ============================================\n",
        "# VISUALIZATION (requires matplotlib)\n",
        "# ============================================\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot evaluation loss\n",
        "    if eval_losses:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(eval_losses, label='Validation Loss', color='orange', alpha=0.7)\n",
        "        plt.xlabel('Evaluation Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Validation Loss Over Time')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./training_metrics.png', dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\nâœ“ Chart saved at: ./training_metrics.png\")\n",
        "    plt.show()\n",
        "\n",
        "except ImportError:\n",
        "    print(f\"\\nâš ï¸  matplotlib not installed, skipping visualization\")\n",
        "    print(f\"   Install with: pip install matplotlib\")\n",
        "\n",
        "# ============================================\n",
        "# DIAGNOSIS\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(\"DIAGNOSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if train_losses and eval_losses:\n",
        "    final_train_loss = train_losses[-1]\n",
        "    final_eval_loss = eval_losses[-1]\n",
        "    gap = final_eval_loss - final_train_loss\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ Overfitting analysis:\")\n",
        "    print(f\"  â€¢ Final training loss: {final_train_loss:.4f}\")\n",
        "    print(f\"  â€¢ Final validation loss: {final_eval_loss:.4f}\")\n",
        "    print(f\"  â€¢ Gap: {gap:.4f}\")\n",
        "\n",
        "    if gap < 0.1:\n",
        "        print(f\"  âœ… Great! No significant overfitting\")\n",
        "    elif gap < 0.3:\n",
        "        print(f\"  âš ï¸  Slight overfitting, but acceptable\")\n",
        "    else:\n",
        "        print(f\"  âŒ Overfitting! Consider:\")\n",
        "        print(f\"     â€¢ Increase dropout\")\n",
        "        print(f\"     â€¢ Reduce number of epochs\")\n",
        "        print(f\"     â€¢ Increase dataset size\")\n",
        "        print(f\"     â€¢ Reduce learning rate\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}